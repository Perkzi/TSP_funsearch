{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1915fced4e72",
   "metadata": {
    "id": "58ba1915fced4e72"
   },
   "source": [
    "# Run FunSearch on Bin Packing\n",
    "Five steps:\n",
    "1. Implement 'LLM' interface.\n",
    "2. Implement a 'SandBox' interface.\n",
    "3. Prepare a 'specification'.\n",
    "4. Prepare a dataset.\n",
    "5. Start FunSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d02b8e9c3ba67",
   "metadata": {
    "id": "6a2d02b8e9c3ba67"
   },
   "source": [
    "## Preparation: download the project file from github. And update system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22453e8153e0934c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22453e8153e0934c",
    "outputId": "d0e02002-cd3b-478f-8f07-02aa86ca656d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython', '/content/funsearch/']\n",
      "Name: openai\n",
      "Version: 1.66.3\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Perkzi/TSP_funsearch.git\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/content/TSP_funsearch/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47175708cc0a93",
   "metadata": {
    "id": "fe47175708cc0a93"
   },
   "source": [
    "## 1. Implement LLM interface\n",
    "Set the API's IP address according to your API provider (See line 65 in the following code).\n",
    "```python\n",
    "conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
    "```\n",
    "You should prepare a 'key' for the LLM API. And fill them in the header (See line 76-80 in the following code).\n",
    "```python\n",
    "headers = {\n",
    "    'Authorization': 'Bearer [put your key here, the key may start with \"sk-...\"]',\n",
    "    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1999e45c9a568b08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1999e45c9a568b08",
    "outputId": "52f9a353-26c8-4381-a9af-084ee2faf418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-BDDBAiq31l66uGXLbfPFZgGesqsXl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def calculate_average_grade(grades):\\n    total = 0\\n    for grade in grades:\\n        total += grade\\n    \\n    average = total / len(grades)\\n    \\n    if average >= 90:\\n        return \"A\"\\n    elif average >= 80:\\n        return \"B\"\\n    elif average >= 70:\\n        return \"C\"\\n    elif average >= 60:\\n        return \"D\"\\n    else:\\n        return \"F\"\\n\\n# Example usage\\ngrades = [85, 92, 78, 65, 90]\\nprint(calculate_average_grade(grades))', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1742488624, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=119, prompt_tokens=45, total_tokens=164, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "    total = 0\n",
      "    for grade in grades:\n",
      "        total += grade\n",
      "    \n",
      "    average = total / len(grades)\n",
      "    \n",
      "    if average >= 90:\n",
      "        return \"A\"\n",
      "    elif average >= 80:\n",
      "        return \"B\"\n",
      "    elif average >= 70:\n",
      "        return \"C\"\n",
      "    elif average >= 60:\n",
      "        return \"D\"\n",
      "    else:\n",
      "        return \"F\"\n",
      "\n",
      "# Example usage\n",
      "grades = [85, 92, 78, 65, 90]\n",
      "print(calculate_average_grade(grades))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import multiprocessing\n",
    "from typing import Collection, Any\n",
    "import http.client\n",
    "import traceback\n",
    "from implementation import sampler\n",
    "\n",
    "\n",
    "\n",
    "def _trim_preface_of_body(sample: str) -> str:\n",
    "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
    "    Please see my comments in sampler.LLM (in sampler.py).\n",
    "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
    "\n",
    "    -Example sample (function & description generated by LLM):\n",
    "    -------------------------------------\n",
    "    This is the optimized function ...\n",
    "    def priority_v2(...) -> ...:\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    -This function removes the description above the function's signature, and the function's signature.\n",
    "    -The indent of the code is preserved.\n",
    "    -Return of this function:\n",
    "    -------------------------------------\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    去掉def之前和自己的行\n",
    "    \"\"\"\n",
    "    lines = sample.splitlines()\n",
    "    func_body_lineno = 0\n",
    "    find_def_declaration = False\n",
    "    for lineno, line in enumerate(lines):\n",
    "        # find the first 'def' statement in the given code\n",
    "        if line[:3] == 'def':\n",
    "            func_body_lineno = lineno\n",
    "            find_def_declaration = True\n",
    "            break\n",
    "    if find_def_declaration:\n",
    "        code = ''\n",
    "        for line in lines[func_body_lineno + 1:]:\n",
    "            code += line + '\\n'\n",
    "        return code\n",
    "    return sample\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from typing import Collection\n",
    "\n",
    "class LLMAPI:\n",
    "    def __init__(self, samples_per_prompt: int, trim=True):\n",
    "        API_KEY = \"sk-CAOVzhWoZ5nyoslq208c0fB912144eB89cD21a0b42E9A211\" # 设置 OpenAI API 密钥\n",
    "        BASE_URL = 'https://api.bltcy.ai/v1'\n",
    "        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "        self._samples_per_prompt = samples_per_prompt\n",
    "        self._trim = trim\n",
    "        self._additional_prompt = (\n",
    "            \"Complete a different and more complex Python function. \"\n",
    "            \"Be creative and you can insert multiple if-else and for-loop in the code logic. \"\n",
    "            \"Only output the Python code, no descriptions.\"\n",
    "        )\n",
    "\n",
    "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
    "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
    "        #print(\"draw-samples\")\n",
    "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
    "\n",
    "    def _draw_sample(self, content: str) -> str:\n",
    "        prompt = '\\n'.join([content, self._additional_prompt])\n",
    "        retries = 0\n",
    "        max_retries = 3\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                #print(\"prompt\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=512,\n",
    "                    stream=False,\n",
    "                )\n",
    "                print(\"response\",response)\n",
    "                output = response.choices[0].message.content\n",
    "\n",
    "                if self._trim:\n",
    "                    output = _trim_preface_of_body(output)\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                retries += 1\n",
    "                time.sleep(2)\n",
    "        print(\"Failed\")\n",
    "        raise RuntimeError(\"Failed after multiple retries\")\n",
    "llm = LLMAPI(samples_per_prompt=4)\n",
    "response = llm._draw_sample('hello')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27817cdec2cedfc",
   "metadata": {
    "id": "d27817cdec2cedfc"
   },
   "source": [
    "## 2. Implement a 'SandBox' interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3d88a87535b6b2",
   "metadata": {
    "id": "3e3d88a87535b6b2"
   },
   "outputs": [],
   "source": [
    "from implementation import evaluator\n",
    "from implementation import evaluator_accelerate\n",
    "\n",
    "\n",
    "class Sandbox(evaluator.Sandbox):\n",
    "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
    "\n",
    "    RZ: Sandbox returns the 'score' of the program and:\n",
    "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
    "    2) stops the execution of the code in time (avoid endless loop).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False, numba_accelerate=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            verbose         : Print evaluate information.\n",
    "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
    "                              support numba acceleration, such as np.piecewise().\n",
    "        \"\"\"\n",
    "        self._verbose = verbose\n",
    "        self._numba_accelerate = numba_accelerate\n",
    "\n",
    "    def run(\n",
    "            self,\n",
    "            program: str,\n",
    "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
    "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
    "            inputs: Any,  # refers to the dataset # self._inputs是数据集的dict，current_input是当前使用的数据集的key (名字str)\n",
    "            test_input: str,  # refers to the current instance\n",
    "            timeout_seconds: int,\n",
    "            **kwargs  # RZ: add this\n",
    "    ) -> tuple[Any, bool]:\n",
    "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
    "\n",
    "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
    "        the output of this function is the score of a given program.\n",
    "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
    "        返回 (function_to_run(test_input)运行结果，True)\n",
    "        \"\"\"\n",
    "        dataset = inputs[test_input]\n",
    "        try:\n",
    "            # 当你有多个进程时，每个进程都会将它们的结果放入各自的 result_queue 中？\n",
    "            result_queue = multiprocessing.Queue()\n",
    "            # 创建一个新的进程 process，目标函数为 self._compile_and_run_function，并传递相应的参数。只并行了一个进程\n",
    "            process = multiprocessing.Process(\n",
    "                target=self._compile_and_run_function,\n",
    "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
    "            )\n",
    "            process.start()\n",
    "            # 等待进程完成。如果进程在 timeout_seconds 时间内未完成，继续执行后续代码\n",
    "            process.join(timeout=timeout_seconds)\n",
    "            if process.is_alive():\n",
    "                # 如果进程仍然在运行（超时未完成）\n",
    "                # if the process is not finished in time, we consider the program illegal\n",
    "                process.terminate()\n",
    "                process.join()\n",
    "                results = None, False\n",
    "            else:\n",
    "                # 如果进程已完成，从队列中获取结果。\n",
    "                if not result_queue.empty():\n",
    "                    results = result_queue.get_nowait()\n",
    "                else:\n",
    "                    results = None, False\n",
    "\n",
    "            return results\n",
    "        except:\n",
    "            return None, False\n",
    "\n",
    "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
    "                                  result_queue):\n",
    "        try:\n",
    "            # optimize the code (decorate function_to_run with @numba.jit())\n",
    "            # 加上 @numba.jit() 装饰器\n",
    "            if numba_accelerate:\n",
    "                program = evaluator_accelerate.add_numba_decorator(\n",
    "                    program=program,\n",
    "                    function_to_evolve=function_to_evolve\n",
    "                )\n",
    "            # compile the program, and maps the global func/var/class name to its address\n",
    "            all_globals_namespace = {}\n",
    "            # execute the program, map func/var/class to global namespace\n",
    "            # exec 是一个内置的Python函数，用于动态执行Python代码。\n",
    "            # program 是一个包含要执行代码的字符串。all_globals_namespace 是一个字典，用于保存执行代码期间创建的全局变量、函数和类\n",
    "            exec(program, all_globals_namespace)\n",
    "            # get the pointer of 'function_to_run'\n",
    "            # 从 all_globals_namespace 中获取要运行的函数\n",
    "            function_to_run = all_globals_namespace[function_to_run]\n",
    "            # return the execution results\n",
    "            results = function_to_run(dataset)\n",
    "            # the results must be int or float\n",
    "            if not isinstance(results, (int, float)):\n",
    "                result_queue.put((None, False))\n",
    "                return\n",
    "            result_queue.put((results, True))\n",
    "        except Exception:\n",
    "            # if raise any exception, we assume the execution failed\n",
    "            result_queue.put((None, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a05827354f9ae",
   "metadata": {
    "id": "ec3a05827354f9ae"
   },
   "source": [
    "## 3. Prepare a 'specification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2f875d128a693a",
   "metadata": {
    "id": "2e2f875d128a693a"
   },
   "outputs": [],
   "source": [
    "from TSP_algorithms import bin_packing_algorithm_0\n",
    "specification = bin_packing_algorithm_0.specification\n",
    "print(specification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bfe61e1661e18",
   "metadata": {
    "id": "391bfe61e1661e18"
   },
   "source": [
    "## 4. Prepare a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea85ccfc8c0ca6d",
   "metadata": {
    "id": "fea85ccfc8c0ca6d"
   },
   "outputs": [],
   "source": [
    "import bin_packing_utils\n",
    "\n",
    "bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66651fb2764ce9",
   "metadata": {
    "id": "cb66651fb2764ce9"
   },
   "source": [
    "## 5. Start FunSearch\n",
    "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e0ec0c796d09ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e0ec0c796d09ca1",
    "outputId": "9722f139-3d8b-4504-9420-3732ef0f3dc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Best score of island 0 increased to -500.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Evaluated Function =================\n",
      "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"Returns priority with which we want to add item to each bin.\n",
      "\n",
      "    Args:\n",
      "        item: Size of item to be added to the bin.\n",
      "        bins: Array of capacities for each bin.\n",
      "\n",
      "    Return:\n",
      "        Array of same size as bins with priority score of each bin.\n",
      "    \"\"\"\n",
      "    ratios = item / bins\n",
      "    log_ratios = np.log(ratios)\n",
      "    priorities = -log_ratios\n",
      "    return priorities\n",
      "------------------------------------------------------\n",
      "Score        : -500.0\n",
      "Sample time  : None\n",
      "Evaluate time: 2.8604772090911865\n",
      "Sample orders: None\n",
      "======================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-BDDBaV2T5IJxkn8veHIrJ1WCLlMAX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v2(item: float, bins: np.ndarray) -> np.ndarray:\\n    priorities = np.zeros_like(bins)\\n    \\n    for i in range(len(bins)):\\n        remaining_space = bins[i] - item\\n        if remaining_space >= 0:\\n            priorities[i] = 1 / (remaining_space + 1)\\n        else:\\n            priorities[i] = np.inf\\n    \\n    return priorities', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1742488650, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=83, prompt_tokens=185, total_tokens=268, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Best score of island 0 increased to -212.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Evaluated Function =================\n",
      "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"Returns priority with which we want to add item to each bin.\n",
      "\n",
      "    Args:\n",
      "        item: Size of item to be added to the bin.\n",
      "        bins: Array of capacities for each bin.\n",
      "\n",
      "    Return:\n",
      "        Array of same size as bins with priority score of each bin.\n",
      "    \"\"\"\n",
      "    priorities = np.zeros_like(bins)\n",
      "    \n",
      "    for i in range(len(bins)):\n",
      "        remaining_space = bins[i] - item\n",
      "        if remaining_space >= 0:\n",
      "            priorities[i] = 1 / (remaining_space + 1)\n",
      "        else:\n",
      "            priorities[i] = np.inf\n",
      "    \n",
      "    return priorities\n",
      "------------------------------------------------------\n",
      "Score        : -212.4\n",
      "Sample time  : 1.3996291160583496\n",
      "Evaluate time: 1.198603630065918\n",
      "Sample orders: 2\n",
      "======================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from implementation import funsearch\n",
    "from implementation import config\n",
    "import dataclasses\n",
    "\n",
    "# It should be noted that the if __name__ == '__main__' is required.\n",
    "# Because the inner code uses multiprocess evaluation.\n",
    "if __name__ == '__main__':\n",
    "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
    "    #config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
    "    config = config.Config(samples_per_prompt=1, evaluate_timeout_seconds=30,programs_database=config.ProgramsDatabaseConfig(num_islands=1))\n",
    "    global_max_sample_num = 2\n",
    "    #global_max_sample_num = 10  # if it is set to None, funsearch will execute an endless loop\n",
    "    funsearch.main(\n",
    "        specification=specification,\n",
    "        inputs=bin_packing_or3,\n",
    "        config=config,\n",
    "        max_sample_nums=global_max_sample_num,\n",
    "        class_config=class_config,\n",
    "        log_dir='../logs/funsearch_llm_api'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
