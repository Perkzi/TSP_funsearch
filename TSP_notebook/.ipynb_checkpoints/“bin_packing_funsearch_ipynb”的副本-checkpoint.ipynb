{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ba1915fced4e72",
   "metadata": {
    "id": "58ba1915fced4e72"
   },
   "source": [
    "# Run FunSearch on Bin Packing\n",
    "Five steps:\n",
    "1. Implement 'LLM' interface.\n",
    "2. Implement a 'SandBox' interface.\n",
    "3. Prepare a 'specification'.\n",
    "4. Prepare a dataset.\n",
    "5. Start FunSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d02b8e9c3ba67",
   "metadata": {
    "id": "6a2d02b8e9c3ba67"
   },
   "source": [
    "## Preparation: download the project file from github. And update system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22453e8153e0934c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22453e8153e0934c",
    "outputId": "81ea926e-72f1-4946-bf40-f6a11d360bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\python311.zip', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\DLLs', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\Lib', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311', '', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\Lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\15201\\\\.conda\\\\envs\\\\python311\\\\Lib\\\\site-packages\\\\Pythonwin', 'D:\\\\cityu课件\\\\ArtifitialIntelligence\\\\project\\\\bin_packing_funsearch']\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/RayZhhh/funsearch.git\n",
    "\n",
    "import sys, os\n",
    "\n",
    "#sys.path.append('/content/funsearch/')\n",
    "# 获取当前工作目录的上一级目录\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# 将上一级目录添加到 sys.path\n",
    "sys.path.append(parent_dir)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47175708cc0a93",
   "metadata": {
    "id": "fe47175708cc0a93"
   },
   "source": [
    "## 1. Implement LLM interface\n",
    "Set the API's IP address according to your API provider (See line 65 in the following code).\n",
    "```python\n",
    "conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
    "```\n",
    "You should prepare a 'key' for the LLM API. And fill them in the header (See line 76-80 in the following code).\n",
    "```python\n",
    "headers = {\n",
    "    'Authorization': 'Bearer [put your key here, the key may start with \"sk-...\"]',\n",
    "    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1999e45c9a568b08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "1999e45c9a568b08",
    "outputId": "3c92edf2-afaf-49eb-8613-4462dbace50c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response ChatCompletion(id='chatcmpl-BDX85BLYdfc8NxX1L2OM3YYDEBw1H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def fibonacci_sequence(n):\\n    fib_list = []\\n    a, b = 0, 1\\n    for i in range(n):\\n        if i == 0:\\n            fib_list.append(a)\\n        elif i == 1:\\n            fib_list.append(b)\\n        else:\\n            fib_list.append(fib_list[i-1] + fib_list[i-2])\\n    return fib_list\\n\\n# Example usage\\nresult = fibonacci_sequence(10)\\nprint(result)', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742565313, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=94, prompt_tokens=45, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "    fib_list = []\n",
      "    a, b = 0, 1\n",
      "    for i in range(n):\n",
      "        if i == 0:\n",
      "            fib_list.append(a)\n",
      "        elif i == 1:\n",
      "            fib_list.append(b)\n",
      "        else:\n",
      "            fib_list.append(fib_list[i-1] + fib_list[i-2])\n",
      "    return fib_list\n",
      "\n",
      "# Example usage\n",
      "result = fibonacci_sequence(10)\n",
      "print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import Collection, Any\n",
    "import http.client\n",
    "import traceback\n",
    "from implementation import sampler\n",
    "\n",
    "\n",
    "\n",
    "def _trim_preface_of_body(sample: str) -> str:\n",
    "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
    "    Please see my comments in sampler.LLM (in sampler.py).\n",
    "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
    "\n",
    "    -Example sample (function & description generated by LLM):\n",
    "    -------------------------------------\n",
    "    This is the optimized function ...\n",
    "    def priority_v2(...) -> ...:\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    -This function removes the description above the function's signature, and the function's signature.\n",
    "    -The indent of the code is preserved.\n",
    "    -Return of this function:\n",
    "    -------------------------------------\n",
    "        return ...\n",
    "    This function aims to ...\n",
    "    -------------------------------------\n",
    "    去掉def之前和自己的行\n",
    "    \"\"\"\n",
    "    lines = sample.splitlines()\n",
    "    func_body_lineno = 0\n",
    "    find_def_declaration = False\n",
    "    for lineno, line in enumerate(lines):\n",
    "        # find the first 'def' statement in the given code\n",
    "        if line[:3] == 'def':\n",
    "            func_body_lineno = lineno\n",
    "            find_def_declaration = True\n",
    "            break\n",
    "    if find_def_declaration:\n",
    "        code = ''\n",
    "        for line in lines[func_body_lineno + 1:]:\n",
    "            code += line + '\\n'\n",
    "        return code\n",
    "    return sample\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from typing import Collection\n",
    "\n",
    "class LLMAPI:\n",
    "    def __init__(self, samples_per_prompt: int, trim=True):\n",
    "        API_KEY = \"sk-CAOVzhWoZ5nyoslq208c0fB912144eB89cD21a0b42E9A211\" # 设置 OpenAI API 密钥\n",
    "        BASE_URL = 'https://api.bltcy.ai/v1'  \n",
    "        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "        self._samples_per_prompt = samples_per_prompt\n",
    "        self._trim = trim\n",
    "        self._additional_prompt = (\n",
    "            \"Complete a different and more complex Python function. \"\n",
    "            \"Be creative and you can insert multiple if-else and for-loop in the code logic. \"\n",
    "            \"Only output the Python code, no descriptions.\"\n",
    "        )\n",
    "\n",
    "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
    "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
    "        #print(\"draw-samples\")\n",
    "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
    "\n",
    "    def _draw_sample(self, content: str) -> str:\n",
    "        prompt = '\\n'.join([content, self._additional_prompt])\n",
    "        retries = 0\n",
    "        max_retries = 3\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                #print(\"prompt\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=512,\n",
    "                    stream=False,\n",
    "                )\n",
    "                print(\"response\",response)\n",
    "                output = response.choices[0].message.content\n",
    "\n",
    "                if self._trim:\n",
    "                    output = _trim_preface_of_body(output)\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                retries += 1\n",
    "                time.sleep(2)\n",
    "        print(\"Failed\")\n",
    "        raise RuntimeError(\"Failed after multiple retries\")\n",
    "llm = LLMAPI(samples_per_prompt=4)\n",
    "response = llm._draw_sample('hello')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27817cdec2cedfc",
   "metadata": {
    "id": "d27817cdec2cedfc"
   },
   "source": [
    "## 2. Implement a 'SandBox' interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3d88a87535b6b2",
   "metadata": {
    "id": "3e3d88a87535b6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 20\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from implementation import evaluator\n",
    "from implementation import evaluator_accelerate\n",
    "\n",
    "\n",
    "class Sandbox(evaluator.Sandbox):\n",
    "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
    "\n",
    "    RZ: Sandbox returns the 'score' of the program and:\n",
    "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
    "    2) stops the execution of the code in time (avoid endless loop).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False, numba_accelerate=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            verbose         : Print evaluate information.\n",
    "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
    "                              support numba acceleration, such as np.piecewise().\n",
    "        \"\"\"\n",
    "        self._verbose = verbose\n",
    "        self._numba_accelerate = numba_accelerate\n",
    "\n",
    "    def run(\n",
    "            self,\n",
    "            program: str,\n",
    "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
    "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
    "            inputs: Any,  # refers to the dataset # self._inputs是数据集的dict，current_input是当前使用的数据集的key (名字str)\n",
    "            test_input: str,  # refers to the current instance\n",
    "            timeout_seconds: int,\n",
    "            **kwargs  # RZ: add this\n",
    "    ) -> tuple[Any, bool]:\n",
    "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
    "\n",
    "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
    "        the output of this function is the score of a given program.\n",
    "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
    "        返回 (function_to_run(test_input)运行结果，True)\n",
    "        \"\"\"\n",
    "        dataset = inputs[test_input]\n",
    "        try:\n",
    "            # 当你有多个进程时，每个进程都会将它们的结果放入各自的 result_queue 中？\n",
    "            result_queue = multiprocessing.Queue()\n",
    "            # 创建一个新的进程 process，目标函数为 self._compile_and_run_function，并传递相应的参数。只并行了一个进程\n",
    "            process = multiprocessing.Process(\n",
    "                target=self._compile_and_run_function,\n",
    "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
    "            )\n",
    "            process.start()\n",
    "            # 等待进程完成。如果进程在 timeout_seconds 时间内未完成，继续执行后续代码\n",
    "            process.join(timeout=timeout_seconds)\n",
    "            if process.is_alive():\n",
    "                # 如果进程仍然在运行（超时未完成）\n",
    "                # if the process is not finished in time, we consider the program illegal\n",
    "                process.terminate()\n",
    "                process.join()\n",
    "                results = None, False\n",
    "            else:\n",
    "                # 如果进程已完成，从队列中获取结果。\n",
    "                if not result_queue.empty():\n",
    "                    results = result_queue.get_nowait()\n",
    "                else:\n",
    "                    results = None, False\n",
    "\n",
    "            return results\n",
    "        except:\n",
    "            return None, False\n",
    "\n",
    "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
    "                                  result_queue):\n",
    "        try:\n",
    "            # optimize the code (decorate function_to_run with @numba.jit())\n",
    "            # 加上 @numba.jit() 装饰器\n",
    "            if numba_accelerate:\n",
    "                program = evaluator_accelerate.add_numba_decorator(\n",
    "                    program=program,\n",
    "                    function_to_evolve=function_to_evolve\n",
    "                )\n",
    "            # compile the program, and maps the global func/var/class name to its address\n",
    "            all_globals_namespace = {}\n",
    "            # execute the program, map func/var/class to global namespace\n",
    "            # exec 是一个内置的Python函数，用于动态执行Python代码。\n",
    "            # program 是一个包含要执行代码的字符串。all_globals_namespace 是一个字典，用于保存执行代码期间创建的全局变量、函数和类\n",
    "            exec(program, all_globals_namespace)\n",
    "            # get the pointer of 'function_to_run'\n",
    "            # 从 all_globals_namespace 中获取要运行的函数\n",
    "            function_to_run = all_globals_namespace[function_to_run]\n",
    "            # return the execution results\n",
    "            results = function_to_run(dataset)\n",
    "            # the results must be int or float\n",
    "            if not isinstance(results, (int, float)):\n",
    "                result_queue.put((None, False))\n",
    "                return\n",
    "            result_queue.put((results, True))\n",
    "        except Exception:\n",
    "            # if raise any exception, we assume the execution failed\n",
    "            result_queue.put((None, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a05827354f9ae",
   "metadata": {
    "id": "ec3a05827354f9ae"
   },
   "source": [
    "## 3. Prepare a 'specification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2f875d128a693a",
   "metadata": {
    "id": "2e2f875d128a693a"
   },
   "outputs": [],
   "source": [
    "specification = r'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_valid_bin_indices(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns indices of bins in which item can fit.\"\"\"\n",
    "    return np.nonzero((bins - item) >= 0)[0]\n",
    "\n",
    "\n",
    "def online_binpack(\n",
    "        items: tuple[float, ...], bins: np.ndarray\n",
    ") -> tuple[list[list[float, ...], ...], np.ndarray]:\n",
    "    \"\"\"Performs online binpacking of `items` into `bins`.\"\"\"\n",
    "    # Track which items are added to each bin.\n",
    "    packing = [[] for _ in bins]\n",
    "    # Add items to bins.\n",
    "    for item in items:\n",
    "        # Extract bins that have sufficient space to fit item.\n",
    "        valid_bin_indices = get_valid_bin_indices(item, bins)\n",
    "        # Score each bin based on heuristic.\n",
    "        priorities = priority(item, bins[valid_bin_indices])\n",
    "        # Add item to bin with highest priority.\n",
    "        best_bin = valid_bin_indices[np.argmax(priorities)]\n",
    "        bins[best_bin] -= item\n",
    "        packing[best_bin].append(item)\n",
    "    # Remove unused bins from packing.\n",
    "    packing = [bin_items for bin_items in packing if bin_items]\n",
    "    return packing, bins\n",
    "\n",
    "\n",
    "@funsearch.run\n",
    "def evaluate(instances: dict) -> float:\n",
    "    \"\"\"Evaluate heuristic function on a set of online binpacking instances.\"\"\"\n",
    "    # List storing number of bins used for each instance.\n",
    "    num_bins = []\n",
    "    # Perform online binpacking for each instance.\n",
    "    for name in instances:\n",
    "        instance = instances[name]\n",
    "        capacity = instance['capacity']\n",
    "        items = instance['items']\n",
    "        # Create num_items bins so there will always be space for all items,\n",
    "        # regardless of packing order. Array has shape (num_items,).\n",
    "        bins = np.array([capacity for _ in range(instance['num_items'])])\n",
    "        # Pack items into bins and return remaining capacity in bins_packed, which\n",
    "        # has shape (num_items,).\n",
    "        _, bins_packed = online_binpack(items, bins)\n",
    "        # If remaining capacity in a bin is equal to initial capacity, then it is\n",
    "        # unused. Count number of used bins.\n",
    "        num_bins.append((bins_packed != capacity).sum())\n",
    "    # Score of heuristic function is negative of average number of bins used\n",
    "    # across instances (as we want to minimize number of bins).\n",
    "    return -np.mean(num_bins)\n",
    "\n",
    "\n",
    "@funsearch.evolve\n",
    "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns priority with which we want to add item to each bin.\n",
    "\n",
    "    Args:\n",
    "        item: Size of item to be added to the bin.\n",
    "        bins: Array of capacities for each bin.\n",
    "\n",
    "    Return:\n",
    "        Array of same size as bins with priority score of each bin.\n",
    "    \"\"\"\n",
    "    ratios = item / bins\n",
    "    log_ratios = np.log(ratios)\n",
    "    priorities = -log_ratios\n",
    "    return priorities\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bfe61e1661e18",
   "metadata": {
    "id": "391bfe61e1661e18"
   },
   "source": [
    "## 4. Prepare a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea85ccfc8c0ca6d",
   "metadata": {
    "id": "fea85ccfc8c0ca6d"
   },
   "outputs": [],
   "source": [
    "import bin_packing_utils\n",
    "\n",
    "bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66651fb2764ce9",
   "metadata": {
    "id": "cb66651fb2764ce9"
   },
   "source": [
    "## 5. Start FunSearch\n",
    "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ec0c796d09ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e0ec0c796d09ca1",
    "outputId": "4549bb7e-1d88-4a7e-cb27-462218fa3381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import numpy as np\n",
      "\n",
      " [Function(name='get_valid_bin_indices', args='item: float, bins: np.ndarray', body='    return np.nonzero((bins - item) >= 0)[0]', return_type='np.ndarray', docstring='Returns indices of bins in which item can fit.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='online_binpack', args='items: tuple[float, ...], bins: np.ndarray', body='    packing = [[] for _ in bins]\\n    # Add items to bins.\\n    for item in items:\\n        # Extract bins that have sufficient space to fit item.\\n        valid_bin_indices = get_valid_bin_indices(item, bins)\\n        # Score each bin based on heuristic.\\n        priorities = priority(item, bins[valid_bin_indices])\\n        # Add item to bin with highest priority.\\n        best_bin = valid_bin_indices[np.argmax(priorities)]\\n        bins[best_bin] -= item\\n        packing[best_bin].append(item)\\n    # Remove unused bins from packing.\\n    packing = [bin_items for bin_items in packing if bin_items]\\n    return packing, bins', return_type='tuple[list[list[float, ...], ...], np.ndarray]', docstring='Performs online binpacking of `items` into `bins`.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='evaluate', args='instances: dict', body=\"    num_bins = []\\n    # Perform online binpacking for each instance.\\n    for name in instances:\\n        instance = instances[name]\\n        capacity = instance['capacity']\\n        items = instance['items']\\n        # Create num_items bins so there will always be space for all items,\\n        # regardless of packing order. Array has shape (num_items,).\\n        bins = np.array([capacity for _ in range(instance['num_items'])])\\n        # Pack items into bins and return remaining capacity in bins_packed, which\\n        # has shape (num_items,).\\n        _, bins_packed = online_binpack(items, bins)\\n        # If remaining capacity in a bin is equal to initial capacity, then it is\\n        # unused. Count number of used bins.\\n        num_bins.append((bins_packed != capacity).sum())\\n    # Score of heuristic function is negative of average number of bins used\\n    # across instances (as we want to minimize number of bins).\\n    return -np.mean(num_bins)\", return_type='float', docstring='Evaluate heuristic function on a set of online binpacking instances.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='priority', args='item: float, bins: np.ndarray', body='    ratios = item / bins\\n    log_ratios = np.log(ratios)\\n    priorities = -log_ratios\\n    return priorities', return_type='np.ndarray', docstring='Returns priority with which we want to add item to each bin.\\n\\n    Args:\\n        item: Size of item to be added to the bin.\\n        bins: Array of capacities for each bin.\\n\\n    Return:\\n        Array of same size as bins with priority score of each bin.\\n    ', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None)] <class 'implementation.code_manipulation.Program'>\n",
      "================= Evaluated Function =================\n",
      "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"Returns priority with which we want to add item to each bin.\n",
      "\n",
      "    Args:\n",
      "        item: Size of item to be added to the bin.\n",
      "        bins: Array of capacities for each bin.\n",
      "\n",
      "    Return:\n",
      "        Array of same size as bins with priority score of each bin.\n",
      "    \"\"\"\n",
      "    ratios = item / bins\n",
      "    log_ratios = np.log(ratios)\n",
      "    priorities = -log_ratios\n",
      "    return priorities\n",
      "------------------------------------------------------\n",
      "Score        : None\n",
      "Sample time  : None\n",
      "Evaluate time: 0.08550500869750977\n",
      "Sample orders: None\n",
      "======================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from implementation import funsearch\n",
    "from implementation import config\n",
    "import dataclasses\n",
    "\n",
    "# It should be noted that the if __name__ == '__main__' is required.\n",
    "# Because the inner code uses multiprocess evaluation.\n",
    "if __name__ == '__main__':\n",
    "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
    "    #config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
    "    config = config.Config(samples_per_prompt=1, evaluate_timeout_seconds=30,programs_database=config.ProgramsDatabaseConfig(num_islands=1))\n",
    "    global_max_sample_num = 2\n",
    "    #global_max_sample_num = 10  # if it is set to None, funsearch will execute an endless loop\n",
    "    funsearch.main(\n",
    "        specification=specification,\n",
    "        inputs=bin_packing_or3,\n",
    "        config=config,\n",
    "        max_sample_nums=global_max_sample_num,\n",
    "        class_config=class_config,\n",
    "        log_dir='../logs/funsearch_llm_api'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python311",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
