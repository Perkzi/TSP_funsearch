{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58ba1915fced4e72",
      "metadata": {
        "id": "58ba1915fced4e72"
      },
      "source": [
        "# Run FunSearch on Bin Packing\n",
        "Five steps:\n",
        "1. Implement 'LLM' interface.\n",
        "2. Implement a 'SandBox' interface.\n",
        "3. Prepare a 'specification'.\n",
        "4. Prepare a dataset.\n",
        "5. Start FunSearch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2d02b8e9c3ba67",
      "metadata": {
        "id": "6a2d02b8e9c3ba67"
      },
      "source": [
        "## Preparation: download the project file from github. And update system path."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import shutil\n",
        "\n",
        "folder_path = \"/content/TSP_funsearch\"\n",
        "shutil.rmtree(folder_path)  # 删除整个文件夹及其所有内容\"\"\""
      ],
      "metadata": {
        "id": "5QDaO7hTacpf"
      },
      "id": "5QDaO7hTacpf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22453e8153e0934c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22453e8153e0934c",
        "outputId": "4118433b-b37d-4406-f2fd-71ca92d656ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TSP_funsearch'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 57 (delta 20), reused 48 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 100.17 KiB | 3.45 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython', '/content/TSP_funsearch/']\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Perkzi/TSP_funsearch.git\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/TSP_funsearch/')\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe47175708cc0a93",
      "metadata": {
        "id": "fe47175708cc0a93"
      },
      "source": [
        "## 1. Implement LLM interface\n",
        "Set the API's IP address according to your API provider (See line 65 in the following code).\n",
        "```python\n",
        "conn = http.client.HTTPSConnection(\"api.chatanywhere.com.cn\")\n",
        "```\n",
        "You should prepare a 'key' for the LLM API. And fill them in the header (See line 76-80 in the following code).\n",
        "```python\n",
        "headers = {\n",
        "    'Authorization': 'Bearer [put your key here, the key may start with \"sk-...\"]',\n",
        "    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1999e45c9a568b08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1999e45c9a568b08",
        "outputId": "4378022e-d6aa-46a7-b835-75a6dc68188f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmP5DRePbOp3vc4efFFQi2ptxAAO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def calculate_fibonacci(n):\\n    fibonacci_series = []\\n    if n <= 0:\\n        return \"Input must be a positive integer\"\\n    elif n == 1:\\n        fibonacci_series.append(0)\\n    elif n == 2:\\n        fibonacci_series.extend([0, 1])\\n    else:\\n        fibonacci_series.extend([0, 1])\\n        for i in range(2, n):\\n            next_num = fibonacci_series[i-1] + fibonacci_series[i-2]\\n            fibonacci_series.append(next_num)\\n    return fibonacci_series\\n\\nprint(calculate_fibonacci(10))', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624027, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=121, prompt_tokens=45, total_tokens=166, completion_tokens_details=None, prompt_tokens_details=None))\n",
            "    fibonacci_series = []\n",
            "    if n <= 0:\n",
            "        return \"Input must be a positive integer\"\n",
            "    elif n == 1:\n",
            "        fibonacci_series.append(0)\n",
            "    elif n == 2:\n",
            "        fibonacci_series.extend([0, 1])\n",
            "    else:\n",
            "        fibonacci_series.extend([0, 1])\n",
            "        for i in range(2, n):\n",
            "            next_num = fibonacci_series[i-1] + fibonacci_series[i-2]\n",
            "            fibonacci_series.append(next_num)\n",
            "    return fibonacci_series\n",
            "\n",
            "print(calculate_fibonacci(10))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import json\n",
        "import multiprocessing\n",
        "from typing import Collection, Any\n",
        "import http.client\n",
        "import traceback\n",
        "from implementation import sampler\n",
        "\n",
        "\n",
        "\n",
        "def _trim_preface_of_body(sample: str) -> str:\n",
        "    \"\"\"Trim the redundant descriptions/symbols/'def' declaration before the function body.\n",
        "    Please see my comments in sampler.LLM (in sampler.py).\n",
        "    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.\n",
        "\n",
        "    -Example sample (function & description generated by LLM):\n",
        "    -------------------------------------\n",
        "    This is the optimized function ...\n",
        "    def priority_v2(...) -> ...:\n",
        "        return ...\n",
        "    This function aims to ...\n",
        "    -------------------------------------\n",
        "    -This function removes the description above the function's signature, and the function's signature.\n",
        "    -The indent of the code is preserved.\n",
        "    -Return of this function:\n",
        "    -------------------------------------\n",
        "        return ...\n",
        "    This function aims to ...\n",
        "    -------------------------------------\n",
        "    去掉def之前和自己的行\n",
        "    \"\"\"\n",
        "    lines = sample.splitlines()\n",
        "    func_body_lineno = 0\n",
        "    find_def_declaration = False\n",
        "    for lineno, line in enumerate(lines):\n",
        "        # find the first 'def' statement in the given code\n",
        "        if line[:3] == 'def':\n",
        "            func_body_lineno = lineno\n",
        "            find_def_declaration = True\n",
        "            break\n",
        "    if find_def_declaration:\n",
        "        code = ''\n",
        "        for line in lines[func_body_lineno + 1:]:\n",
        "            code += line + '\\n'\n",
        "        return code\n",
        "    return sample\n",
        "\n",
        "from openai import OpenAI\n",
        "import time\n",
        "from typing import Collection\n",
        "#from TSP_algorithms.additional_prompts import base_prompt as ap  # choose a different additional_prompts\n",
        "from TSP_algorithms.additional_prompts2 import step_by_step_prompts as ap\n",
        "\n",
        "class LLMAPI:\n",
        "    def __init__(self, samples_per_prompt: int, trim=True):\n",
        "        API_KEY = \"\" # 设置 OpenAI API 密钥\n",
        "        BASE_URL = 'https://api.bltcy.ai/v1'\n",
        "        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
        "        self._samples_per_prompt = samples_per_prompt\n",
        "        self._trim = trim\n",
        "        if ap:\n",
        "            self._additional_prompt = ap\n",
        "        else:\n",
        "            self._additional_prompt = (\n",
        "                \"Complete a different and more complex Python function. \"\n",
        "                \"Be creative and you can insert multiple if-else and for-loop in the code logic. \"\n",
        "                \"Only output the Python code, no descriptions.\"\n",
        "            )\n",
        "        if type(self._additional_prompt) is not list:\n",
        "          self._additional_prompt = [self._additional_prompt]\n",
        "\n",
        "    def draw_samples(self, prompt: str) -> Collection[str]:\n",
        "        \"\"\"Returns multiple predicted continuations of `prompt`.\"\"\"\n",
        "        #print(\"draw-samples\")\n",
        "        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]\n",
        "\n",
        "    def _draw_sample(self, content: str) -> str:\n",
        "        max_retries = 3\n",
        "\n",
        "        # whether have the pre prompt before main prompt\n",
        "        if len(self._additional_prompt)<=1:\n",
        "          prompt = '\\n'.join([content, self._additional_prompt[0]])\n",
        "        elif len(self._additional_prompt)==2:\n",
        "          pre_prompt = '\\n'.join([content, self._additional_prompt[0]])\n",
        "          retries = 0\n",
        "          pre_output = ''\n",
        "          prompt = ''\n",
        "          while retries < max_retries:\n",
        "            try:\n",
        "                #print(\"prompt\")\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": pre_prompt}],\n",
        "                    max_tokens=512,\n",
        "                    stream=False,\n",
        "                )\n",
        "                print(\"pre_response\",response)\n",
        "                pre_output = response.choices[0].message.content\n",
        "                # 将 第二个prompt中的 <insert optimization analysis here> 替换为分析结果。\n",
        "                prompt = '\\n'.join([content, self._additional_prompt[1].replace(\"<insert optimization analysis here>\", pre_output)])\n",
        "                break\n",
        "                #return output\n",
        "            except Exception as e:\n",
        "                traceback.print_exc()\n",
        "                print(f\"Error occurred: {e}\")\n",
        "                retries += 1\n",
        "                time.sleep(2)\n",
        "        else:\n",
        "          raise RuntimeError(\"list of additional_prompt should have length <= 2\")\n",
        "\n",
        "        # the main prompt\n",
        "        retries = 0\n",
        "        output = ''\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                #print(\"prompt\")\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=512,\n",
        "                    stream=False,\n",
        "                )\n",
        "                print(\"response\",response)\n",
        "                output = response.choices[0].message.content\n",
        "\n",
        "                if self._trim:\n",
        "                    output = _trim_preface_of_body(output)\n",
        "                break\n",
        "                #return output\n",
        "            except Exception as e:\n",
        "                traceback.print_exc()\n",
        "                print(f\"Error occurred: {e}\")\n",
        "                retries += 1\n",
        "                time.sleep(2)\n",
        "        if output == '':\n",
        "            print(\"Failed\")\n",
        "            raise RuntimeError(\"Failed after multiple retries\")\n",
        "        return output\n",
        "\n",
        "\n",
        "llm = LLMAPI(samples_per_prompt=4)\n",
        "response = llm._draw_sample('hello')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27817cdec2cedfc",
      "metadata": {
        "id": "d27817cdec2cedfc"
      },
      "source": [
        "## 2. Implement a 'SandBox' interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3d88a87535b6b2",
      "metadata": {
        "id": "3e3d88a87535b6b2"
      },
      "outputs": [],
      "source": [
        "from implementation import evaluator\n",
        "from implementation import evaluator_accelerate\n",
        "\n",
        "\n",
        "class Sandbox(evaluator.Sandbox):\n",
        "    \"\"\"Sandbox for executing generated code. Implemented by RZ.\n",
        "\n",
        "    RZ: Sandbox returns the 'score' of the program and:\n",
        "    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).\n",
        "    2) stops the execution of the code in time (avoid endless loop).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False, numba_accelerate=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            verbose         : Print evaluate information.\n",
        "            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions\n",
        "                              support numba acceleration, such as np.piecewise().\n",
        "        \"\"\"\n",
        "        self._verbose = verbose\n",
        "        self._numba_accelerate = numba_accelerate\n",
        "\n",
        "    def run(\n",
        "            self,\n",
        "            program: str,\n",
        "            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')\n",
        "            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.\n",
        "            inputs: Any,  # refers to the dataset # self._inputs是数据集的dict，current_input是当前使用的数据集的key (名字str)\n",
        "            test_input: str,  # refers to the current instance\n",
        "            timeout_seconds: int,\n",
        "            **kwargs  # RZ: add this\n",
        "    ) -> tuple[Any, bool]:\n",
        "        \"\"\"Returns `function_to_run(test_input)` and whether execution succeeded.\n",
        "\n",
        "        RZ: If the generated code (generated by LLM) is executed successfully,\n",
        "        the output of this function is the score of a given program.\n",
        "        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.\n",
        "        返回 (function_to_run(test_input)运行结果，True)\n",
        "        \"\"\"\n",
        "        dataset = inputs[test_input]\n",
        "        try:\n",
        "            # 当你有多个进程时，每个进程都会将它们的结果放入各自的 result_queue 中？\n",
        "            result_queue = multiprocessing.Queue()\n",
        "            # 创建一个新的进程 process，目标函数为 self._compile_and_run_function，并传递相应的参数。只并行了一个进程\n",
        "            process = multiprocessing.Process(\n",
        "                target=self._compile_and_run_function,\n",
        "                args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)\n",
        "            )\n",
        "            process.start()\n",
        "            # 等待进程完成。如果进程在 timeout_seconds 时间内未完成，继续执行后续代码\n",
        "            process.join(timeout=timeout_seconds)\n",
        "            if process.is_alive():\n",
        "                # 如果进程仍然在运行（超时未完成）\n",
        "                # if the process is not finished in time, we consider the program illegal\n",
        "                process.terminate()\n",
        "                process.join()\n",
        "                results = None, False\n",
        "            else:\n",
        "                # 如果进程已完成，从队列中获取结果。\n",
        "                if not result_queue.empty():\n",
        "                    results = result_queue.get_nowait()\n",
        "                else:\n",
        "                    results = None, False\n",
        "\n",
        "            return results\n",
        "        except:\n",
        "            return None, False\n",
        "\n",
        "    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,\n",
        "                                  result_queue):\n",
        "        try:\n",
        "            # optimize the code (decorate function_to_run with @numba.jit())\n",
        "            # 加上 @numba.jit() 装饰器\n",
        "            if numba_accelerate:\n",
        "                program = evaluator_accelerate.add_numba_decorator(\n",
        "                    program=program,\n",
        "                    function_to_evolve=function_to_evolve\n",
        "                )\n",
        "            # compile the program, and maps the global func/var/class name to its address\n",
        "            all_globals_namespace = {}\n",
        "            # execute the program, map func/var/class to global namespace\n",
        "            # exec 是一个内置的Python函数，用于动态执行Python代码。\n",
        "            # program 是一个包含要执行代码的字符串。all_globals_namespace 是一个字典，用于保存执行代码期间创建的全局变量、函数和类\n",
        "            exec(program, all_globals_namespace)\n",
        "            # get the pointer of 'function_to_run'\n",
        "            # 从 all_globals_namespace 中获取要运行的函数\n",
        "            function_to_run = all_globals_namespace[function_to_run]\n",
        "            # return the execution results\n",
        "            results = function_to_run(dataset)\n",
        "            # the results must be int or float\n",
        "            if not isinstance(results, (int, float)):\n",
        "                result_queue.put((None, False))\n",
        "                return\n",
        "            result_queue.put((results, True))\n",
        "        except Exception:\n",
        "            # if raise any exception, we assume the execution failed\n",
        "            result_queue.put((None, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3a05827354f9ae",
      "metadata": {
        "id": "ec3a05827354f9ae"
      },
      "source": [
        "## 3. Prepare a 'specification'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2f875d128a693a",
      "metadata": {
        "id": "2e2f875d128a693a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026f2586-56f7-4eb1-cc4c-ed4fe6fbb668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "def get_valid_bin_indices(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns indices of bins in which item can fit.\"\"\"\n",
            "    return np.nonzero((bins - item) >= 0)[0]\n",
            "\n",
            "\n",
            "def online_binpack(\n",
            "        items: tuple[float, ...], bins: np.ndarray\n",
            ") -> tuple[list[list[float, ...], ...], np.ndarray]:\n",
            "    \"\"\"Performs online binpacking of `items` into `bins`.\"\"\"\n",
            "    # Track which items are added to each bin.\n",
            "    packing = [[] for _ in bins]\n",
            "    # Add items to bins.\n",
            "    for item in items:\n",
            "        # Extract bins that have sufficient space to fit item.\n",
            "        valid_bin_indices = get_valid_bin_indices(item, bins)\n",
            "        # Score each bin based on heuristic.\n",
            "        priorities = priority(item, bins[valid_bin_indices])\n",
            "        # Add item to bin with highest priority.\n",
            "        best_bin = valid_bin_indices[np.argmax(priorities)]\n",
            "        bins[best_bin] -= item\n",
            "        packing[best_bin].append(item)\n",
            "    # Remove unused bins from packing.\n",
            "    packing = [bin_items for bin_items in packing if bin_items]\n",
            "    return packing, bins\n",
            "\n",
            "\n",
            "@funsearch.run\n",
            "def evaluate(instances: dict) -> float:\n",
            "    \"\"\"Evaluate heuristic function on a set of online binpacking instances.\"\"\"\n",
            "    # List storing number of bins used for each instance.\n",
            "    num_bins = []\n",
            "    # Perform online binpacking for each instance.\n",
            "    for name in instances:\n",
            "        instance = instances[name]\n",
            "        capacity = instance['capacity']\n",
            "        items = instance['items']\n",
            "        # Create num_items bins so there will always be space for all items,\n",
            "        # regardless of packing order. Array has shape (num_items,).\n",
            "        bins = np.array([capacity for _ in range(instance['num_items'])])\n",
            "        # Pack items into bins and return remaining capacity in bins_packed, which\n",
            "        # has shape (num_items,).\n",
            "        _, bins_packed = online_binpack(items, bins)\n",
            "        # If remaining capacity in a bin is equal to initial capacity, then it is\n",
            "        # unused. Count number of used bins.\n",
            "        num_bins.append((bins_packed != capacity).sum())\n",
            "    # Score of heuristic function is negative of average number of bins used\n",
            "    # across instances (as we want to minimize number of bins).\n",
            "    return -np.mean(num_bins)\n",
            "\n",
            "\n",
            "@funsearch.evolve\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    log_ratios = np.log(ratios)\n",
            "    priorities = -log_ratios\n",
            "    return priorities\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from TSP_algorithms import bin_packing_algorithm_0\n",
        "specification = bin_packing_algorithm_0.specification\n",
        "print(specification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391bfe61e1661e18",
      "metadata": {
        "id": "391bfe61e1661e18"
      },
      "source": [
        "## 4. Prepare a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea85ccfc8c0ca6d",
      "metadata": {
        "id": "fea85ccfc8c0ca6d"
      },
      "outputs": [],
      "source": [
        "import bin_packing_utils\n",
        "\n",
        "bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb66651fb2764ce9",
      "metadata": {
        "id": "cb66651fb2764ce9"
      },
      "source": [
        "## 5. Start FunSearch\n",
        "Please note that in jupyter notebook the following code will fail. This is because juypter does not support multiprocessing. Colab backend supports multiprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0ec0c796d09ca1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e0ec0c796d09ca1",
        "outputId": "78cbf892-fb18-4e60-c4f1-c9536703fe22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "import numpy as np\n",
            "\n",
            " [Function(name='get_valid_bin_indices', args='item: float, bins: np.ndarray', body='    return np.nonzero((bins - item) >= 0)[0]', return_type='np.ndarray', docstring='Returns indices of bins in which item can fit.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='online_binpack', args='items: tuple[float, ...], bins: np.ndarray', body='    packing = [[] for _ in bins]\\n    # Add items to bins.\\n    for item in items:\\n        # Extract bins that have sufficient space to fit item.\\n        valid_bin_indices = get_valid_bin_indices(item, bins)\\n        # Score each bin based on heuristic.\\n        priorities = priority(item, bins[valid_bin_indices])\\n        # Add item to bin with highest priority.\\n        best_bin = valid_bin_indices[np.argmax(priorities)]\\n        bins[best_bin] -= item\\n        packing[best_bin].append(item)\\n    # Remove unused bins from packing.\\n    packing = [bin_items for bin_items in packing if bin_items]\\n    return packing, bins', return_type='tuple[list[list[float, ...], ...], np.ndarray]', docstring='Performs online binpacking of `items` into `bins`.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='evaluate', args='instances: dict', body=\"    num_bins = []\\n    # Perform online binpacking for each instance.\\n    for name in instances:\\n        instance = instances[name]\\n        capacity = instance['capacity']\\n        items = instance['items']\\n        # Create num_items bins so there will always be space for all items,\\n        # regardless of packing order. Array has shape (num_items,).\\n        bins = np.array([capacity for _ in range(instance['num_items'])])\\n        # Pack items into bins and return remaining capacity in bins_packed, which\\n        # has shape (num_items,).\\n        _, bins_packed = online_binpack(items, bins)\\n        # If remaining capacity in a bin is equal to initial capacity, then it is\\n        # unused. Count number of used bins.\\n        num_bins.append((bins_packed != capacity).sum())\\n    # Score of heuristic function is negative of average number of bins used\\n    # across instances (as we want to minimize number of bins).\\n    return -np.mean(num_bins)\", return_type='float', docstring='Evaluate heuristic function on a set of online binpacking instances.', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None), Function(name='priority', args='item: float, bins: np.ndarray', body='    ratios = item / bins\\n    log_ratios = np.log(ratios)\\n    priorities = -log_ratios\\n    return priorities', return_type='np.ndarray', docstring='Returns priority with which we want to add item to each bin.\\n\\n    Args:\\n        item: Size of item to be added to the bin.\\n        bins: Array of capacities for each bin.\\n\\n    Return:\\n        Array of same size as bins with priority score of each bin.\\n    ', score=None, global_sample_nums=None, sample_time=None, evaluate_time=None)] <class 'implementation.code_manipulation.Program'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Best score of island 0 increased to -500.0\n",
            "INFO:absl:Best score of island 1 increased to -500.0\n",
            "INFO:absl:Best score of island 2 increased to -500.0\n",
            "INFO:absl:Best score of island 3 increased to -500.0\n",
            "INFO:absl:Best score of island 4 increased to -500.0\n",
            "INFO:absl:Best score of island 5 increased to -500.0\n",
            "INFO:absl:Best score of island 6 increased to -500.0\n",
            "INFO:absl:Best score of island 7 increased to -500.0\n",
            "INFO:absl:Best score of island 8 increased to -500.0\n",
            "INFO:absl:Best score of island 9 increased to -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    log_ratios = np.log(ratios)\n",
            "    priorities = -log_ratios\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -500.0\n",
            "Sample time  : None\n",
            "Evaluate time: 1.0512545108795166\n",
            "Sample orders: None\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTHF5LBVLcIQwOhdVQFMUbfBcQR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef priority_v1(item: float, bins: np.ndarray) -> np.ndarray:\\n    ratios = item / bins\\n    priorities = []\\n    \\n    for i, ratio in enumerate(ratios):\\n        if ratio < 0.2:\\n            priority = 1 / (1 + np.exp(-ratio))\\n        elif ratio >= 0.2 and ratio < 0.5:\\n            priority = np.square(ratio)\\n        else:\\n            priority = np.tanh(ratio)\\n        \\n        priorities.append(priority)\\n    \\n    return np.array(priorities)\\n```', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624287, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=118, prompt_tokens=185, total_tokens=303, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTJl5nzJ1uGGhrOsPxpAqhxAVS4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v1(item: float, bins: np.ndarray) -> np.ndarray:\\n    priorities = np.zeros_like(bins)\\n    \\n    for i in range(len(bins)):\\n        if item > bins[i]:\\n            priorities[i] = 0\\n        elif item == bins[i]:\\n            priorities[i] = 10\\n        else:\\n            priorities[i] = (bins[i] - item) / bins[i] * 100\\n            \\n    return priorities', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624289, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=92, prompt_tokens=185, total_tokens=277, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTK1B2cAvztpxAWzeWcLeADZl2e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef priority_v1(item: float, bins: np.ndarray) -> np.ndarray:\\n    ratios = item / bins\\n    priorities = np.zeros(len(bins))\\n    \\n    for i in range(len(bins)):\\n        if ratios[i] > 0.5:\\n            priorities[i] = 2 * ratios[i]\\n        elif ratios[i] > 0.3:\\n            priorities[i] = 1.5 * ratios[i]\\n        else:\\n            priorities[i] = ratios[i] * np.log(ratios[i])\\n    \\n    return priorities\\n```', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624290, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=117, prompt_tokens=185, total_tokens=302, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTMAB89eYBOHbJJWu2ztYjGxI5s', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v1(item: float, bins: np.ndarray) -> np.ndarray:\\n    priorities = []\\n    total_capacity = np.sum(bins)\\n    \\n    for bin_capacity in bins:\\n        if bin_capacity >= item:\\n            priorities.append(0.0)\\n        else:\\n            remaining_capacity = total_capacity - bin_capacity\\n            ratio = item / remaining_capacity\\n            priority = np.log(ratio)\\n            priorities.append(priority)\\n    \\n    return np.array(priorities)', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624292, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=96, prompt_tokens=185, total_tokens=281, completion_tokens_details=None, prompt_tokens_details=None))\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = []\n",
            "    \n",
            "    for i, ratio in enumerate(ratios):\n",
            "        if ratio < 0.2:\n",
            "            priority = 1 / (1 + np.exp(-ratio))\n",
            "        elif ratio >= 0.2 and ratio < 0.5:\n",
            "            priority = np.square(ratio)\n",
            "        else:\n",
            "            priority = np.tanh(ratio)\n",
            "        \n",
            "        priorities.append(priority)\n",
            "    \n",
            "    return np.array(priorities)\n",
            "------------------------------------------------------\n",
            "Score        : None\n",
            "Sample time  : 1.4548338651657104\n",
            "Evaluate time: 0.5289671421051025\n",
            "Sample orders: 3\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = np.zeros_like(bins)\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if item > bins[i]:\n",
            "            priorities[i] = 0\n",
            "        elif item == bins[i]:\n",
            "            priorities[i] = 10\n",
            "        else:\n",
            "            priorities[i] = (bins[i] - item) / bins[i] * 100\n",
            "            \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -500.0\n",
            "Sample time  : 1.4548338651657104\n",
            "Evaluate time: 0.9822883605957031\n",
            "Sample orders: 4\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Best score of island 5 increased to -211.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = np.zeros(len(bins))\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if ratios[i] > 0.5:\n",
            "            priorities[i] = 2 * ratios[i]\n",
            "        elif ratios[i] > 0.3:\n",
            "            priorities[i] = 1.5 * ratios[i]\n",
            "        else:\n",
            "            priorities[i] = ratios[i] * np.log(ratios[i])\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -211.95\n",
            "Sample time  : 1.4548338651657104\n",
            "Evaluate time: 1.103666067123413\n",
            "Sample orders: 5\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    priorities = []\n",
            "    total_capacity = np.sum(bins)\n",
            "    \n",
            "    for bin_capacity in bins:\n",
            "        if bin_capacity >= item:\n",
            "            priorities.append(0.0)\n",
            "        else:\n",
            "            remaining_capacity = total_capacity - bin_capacity\n",
            "            ratio = item / remaining_capacity\n",
            "            priority = np.log(ratio)\n",
            "            priorities.append(priority)\n",
            "    \n",
            "    return np.array(priorities)\n",
            "------------------------------------------------------\n",
            "Score        : -212.75\n",
            "Sample time  : 1.4548338651657104\n",
            "Evaluate time: 1.050858974456787\n",
            "Sample orders: 6\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTQqLPnQRsBTr78GEB2wXtRlEjP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v2(item: float, bins: np.ndarray) -> np.ndarray:\\n    \"\"\"Advanced version of priority calculation.\"\"\"\\n    ratios = item / bins\\n    priorities = np.zeros(len(bins))\\n\\n    for i in range(len(bins)):\\n        if ratios[i] > 0.7:\\n            priorities[i] = 2.5 * ratios[i]\\n        elif ratios[i] > 0.5:\\n            priorities[i] = 2 * ratios[i]\\n        elif ratios[i] > 0.3:\\n            priorities[i] = 1.5 * ratios[i]\\n        else:\\n            priorities[i] = ratios[i] * np.log(ratios[i])\\n\\n    return priorities', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624296, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=142, prompt_tokens=375, total_tokens=517, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTSWVJeqhxcHuZG5UN9EMlOIOOG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v2(item: float, bins: np.ndarray) -> np.ndarray:\\n    \"\"\"Even more improved version of priorities.\"\"\"\\n    ratios = item / bins\\n    priorities = np.zeros(len(bins))\\n    \\n    for i in range(len(bins)):\\n        if ratios[i] > 0.7:\\n            priorities[i] = 3 * ratios[i]\\n        elif ratios[i] > 0.5:\\n            priorities[i] = 2.5 * ratios[i]\\n        elif ratios[i] > 0.3:\\n            priorities[i] = 2 * ratios[i]\\n        else:\\n            priorities[i] = ratios[i] * np.log(ratios[i])\\n    \\n    for i in range(len(bins)):\\n        if priorities[i] < 1:\\n            priorities[i] = 1 + np.exp(priorities[i])\\n            \\n    return priorities', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624298, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=177, prompt_tokens=375, total_tokens=552, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTWfR0DBjM5AWzmseG24Xr5yMLL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v2(item: float, bins: np.ndarray) -> np.ndarray:\\n    ratios = item / bins\\n    priorities = np.zeros(len(bins))\\n    \\n    for i in range(len(bins)):\\n        if ratios[i] > 0.7:\\n            priorities[i] = 3 * ratios[i]\\n        elif ratios[i] > 0.5:\\n            priorities[i] = 2 * ratios[i]\\n        elif ratios[i] > 0.3:\\n            priorities[i] = 1.5 * ratios[i]\\n        else:\\n            priorities[i] = ratios[i] * np.log(ratios[i])\\n        \\n        if bins[i] < 10:\\n            priorities[i] *= 1.2\\n        \\n    return priorities', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624302, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=153, prompt_tokens=375, total_tokens=528, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.bltcy.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response ChatCompletion(id='chatcmpl-BDmTXwdzIVvq99g7eqf4Mjgfm9AVd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='def priority_v2(item: float, bins: np.ndarray) -> np.ndarray:\\n    ratios = item / bins\\n    priorities = np.zeros(len(bins))\\n    \\n    for i in range(len(bins)):\\n        if ratios[i] > 0.7:\\n            priorities[i] = 2.5 * ratios[i]\\n        elif ratios[i] > 0.5:\\n            priorities[i] = 2 * ratios[i]\\n        elif ratios[i] > 0.3:\\n            priorities[i] = 1.5 * ratios[i]\\n        elif ratios[i] > 0.1:\\n            priorities[i] = ratios[i] * np.log(ratios[i])\\n        else:\\n            priorities[i] = 0.5 * ratios[i] + np.sqrt(ratios[i])\\n    \\n    return priorities', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1742624303, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint='fp_0165350fbb', usage=CompletionUsage(completion_tokens=168, prompt_tokens=375, total_tokens=543, completion_tokens_details=None, prompt_tokens_details=None))\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    \"\"\"Advanced version of priority calculation.\"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = np.zeros(len(bins))\n",
            "\n",
            "    for i in range(len(bins)):\n",
            "        if ratios[i] > 0.7:\n",
            "            priorities[i] = 2.5 * ratios[i]\n",
            "        elif ratios[i] > 0.5:\n",
            "            priorities[i] = 2 * ratios[i]\n",
            "        elif ratios[i] > 0.3:\n",
            "            priorities[i] = 1.5 * ratios[i]\n",
            "        else:\n",
            "            priorities[i] = ratios[i] * np.log(ratios[i])\n",
            "\n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -211.95\n",
            "Sample time  : 2.150103509426117\n",
            "Evaluate time: 1.1175134181976318\n",
            "Sample orders: 7\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    \"\"\"Even more improved version of priorities.\"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = np.zeros(len(bins))\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if ratios[i] > 0.7:\n",
            "            priorities[i] = 3 * ratios[i]\n",
            "        elif ratios[i] > 0.5:\n",
            "            priorities[i] = 2.5 * ratios[i]\n",
            "        elif ratios[i] > 0.3:\n",
            "            priorities[i] = 2 * ratios[i]\n",
            "        else:\n",
            "            priorities[i] = ratios[i] * np.log(ratios[i])\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if priorities[i] < 1:\n",
            "            priorities[i] = 1 + np.exp(priorities[i])\n",
            "            \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -231.2\n",
            "Sample time  : 2.150103509426117\n",
            "Evaluate time: 1.1793272495269775\n",
            "Sample orders: 8\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = np.zeros(len(bins))\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if ratios[i] > 0.7:\n",
            "            priorities[i] = 3 * ratios[i]\n",
            "        elif ratios[i] > 0.5:\n",
            "            priorities[i] = 2 * ratios[i]\n",
            "        elif ratios[i] > 0.3:\n",
            "            priorities[i] = 1.5 * ratios[i]\n",
            "        else:\n",
            "            priorities[i] = ratios[i] * np.log(ratios[i])\n",
            "        \n",
            "        if bins[i] < 10:\n",
            "            priorities[i] *= 1.2\n",
            "        \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -211.95\n",
            "Sample time  : 2.150103509426117\n",
            "Evaluate time: 1.1163477897644043\n",
            "Sample orders: 9\n",
            "======================================================\n",
            "\n",
            "\n",
            "================= Evaluated Function =================\n",
            "def priority(item: float, bins: np.ndarray) -> np.ndarray:\n",
            "    \"\"\"Returns priority with which we want to add item to each bin.\n",
            "\n",
            "    Args:\n",
            "        item: Size of item to be added to the bin.\n",
            "        bins: Array of capacities for each bin.\n",
            "\n",
            "    Return:\n",
            "        Array of same size as bins with priority score of each bin.\n",
            "    \"\"\"\n",
            "    ratios = item / bins\n",
            "    priorities = np.zeros(len(bins))\n",
            "    \n",
            "    for i in range(len(bins)):\n",
            "        if ratios[i] > 0.7:\n",
            "            priorities[i] = 2.5 * ratios[i]\n",
            "        elif ratios[i] > 0.5:\n",
            "            priorities[i] = 2 * ratios[i]\n",
            "        elif ratios[i] > 0.3:\n",
            "            priorities[i] = 1.5 * ratios[i]\n",
            "        elif ratios[i] > 0.1:\n",
            "            priorities[i] = ratios[i] * np.log(ratios[i])\n",
            "        else:\n",
            "            priorities[i] = 0.5 * ratios[i] + np.sqrt(ratios[i])\n",
            "    \n",
            "    return priorities\n",
            "------------------------------------------------------\n",
            "Score        : -211.95\n",
            "Sample time  : 2.150103509426117\n",
            "Evaluate time: 1.1289806365966797\n",
            "Sample orders: 10\n",
            "======================================================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from implementation import funsearch\n",
        "from implementation import config\n",
        "import dataclasses\n",
        "\n",
        "# It should be noted that the if __name__ == '__main__' is required.\n",
        "# Because the inner code uses multiprocess evaluation.\n",
        "if __name__ == '__main__':\n",
        "    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)\n",
        "    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)\n",
        "    #config = config.Config(samples_per_prompt=1, evaluate_timeout_seconds=30,programs_database=config.ProgramsDatabaseConfig(num_islands=1))\n",
        "    #global_max_sample_num = 2\n",
        "    global_max_sample_num = 10  # if it is set to None, funsearch will execute an endless loop\n",
        "    funsearch.main(\n",
        "        specification=specification,\n",
        "        inputs=bin_packing_or3,\n",
        "        config=config,\n",
        "        max_sample_nums=global_max_sample_num,\n",
        "        class_config=class_config,\n",
        "        log_dir='../logs/funsearch_llm_api'\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}